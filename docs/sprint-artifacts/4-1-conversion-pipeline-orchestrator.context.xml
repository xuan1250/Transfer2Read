<story-context id="4-1-conversion-pipeline-orchestrator" v="2.0">
  <metadata>
    <epicId>epic-4</epicId>
    <storyId>4-1</storyId>
    <title>Conversion Pipeline Orchestrator</title>
    <status>ready-for-dev</status>
    <generatedAt>2026-01-07</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/4-1-conversion-pipeline-orchestrator.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>Developer</asA>
    <iWant>to implement the main conversion workflow using Celery and the existing StirlingPDFClient</iWant>
    <soThat>PDFs are reliably converted to HTML and then processed through the AI pipeline</soThat>
    <tasks>
      - Infrastructure Setup: Deploy Stirling-PDF service on Railway, configure internal networking, set STIRLING_PDF_URL and STIRLING_PDF_API_KEY environment variables
      - Define Celery Tasks: Implement task_convert_pdf_to_html, task_extract_content, task_analyze_structure, task_generate_epub with proper signatures
      - Implement Pipeline Orchestration: Create conversion workflow chain in backend/app/services/conversion/pipeline.py
      - State Updates &amp; Monitoring: Use update_job_status helper for progress tracking at each stage
      - Testing: Unit tests with mocked StirlingPDFClient and Supabase, integration test for full chain execution
    </tasks>
  </story>

  <acceptanceCriteria>
    1. Celery Workflow Orchestration: Implement Celery Canvas chain executing tasks in order (convert, extract, analyze, generate), pass job_id and intermediate artifacts between tasks
    2. Job State Management: Update conversion_jobs table in Supabase at each stage transition (status, progress 0-100%, stage_metadata JSONB field), use Redis for transient state
    3. Stirling-PDF Integration: Implement task_convert_pdf_to_html that downloads PDF from Supabase Storage, calls StirlingPDFClient.convert_pdf_to_html(), saves HTML output, updates job status to analyzing with progress 25%, handle timeouts with soft_time_limit=300 and time_limit=360
    4. Error Handling &amp; Resilience: Implement automatic retries for transient failures (autoretry_for=(httpx.HTTPError, httpx.TimeoutException), retry_kwargs={'max_retries': 3, 'countdown': 60}), implement global error handler (on_failure callback) that marks job as failed with error details
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/sprint-artifacts/tech-spec-epic-4.md</path>
        <title>Technical Specification: Epic 4 (AI-Powered Conversion Engine)</title>
        <section>Implementation Details > Technical Approach > Pipeline Architecture</section>
        <snippet>
          Celery chain pattern: workflow = chain(analyze_layout.s(job_id), extract_content.s(), identify_structure.s(), generate_epub.s(), calculate_quality_score.s()). Each task updates conversion_jobs table with current status and progress %.
        </snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture - API-First Intelligence</title>
        <section>Novel Pattern Designs > PDF Conversion Pipeline (Async)</section>
        <snippet>
          HTML-First Hybrid approach using Stirling-PDF for high-fidelity HTML conversion + GPT-4o for semantic structure analysis. Long-running tasks managed by Celery workers backed by Redis. All data managed through Supabase.
        </snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture - Deployment</title>
        <section>Deployment Architecture > Railway Services</section>
        <snippet>
          Backend API + Worker deployed to Railway. Worker container runs Celery worker processing conversion jobs. Redis managed service for Celery message broker.
        </snippet>
      </doc>
    </docs>
    <code>
      <file>
        <path>backend/app/tasks/conversion_pipeline.py</path>
        <kind>celery_tasks</kind>
        <symbol>convert_to_html, extract_content, identify_structure, generate_epub, calculate_quality_score</symbol>
        <lines>1-1005</lines>
        <reason>FULLY IMPLEMENTED - Complete pipeline with 5 tasks. Use as reference for understanding existing implementation patterns and error handling strategies.</reason>
      </file>
      <file>
        <path>backend/app/services/stirling/stirling_client.py</path>
        <kind>client</kind>
        <symbol>StirlingPDFClient</symbol>
        <lines>1-91</lines>
        <reason>EXISTING CLIENT - Ready to use. Methods: convert_pdf_to_html(pdf_bytes, filename) returns HTML string, get_version() for health check. Configured with STIRLING_PDF_URL and STIRLING_PDF_API_KEY from settings.</reason>
      </file>
      <file>
        <path>backend/app/services/job_service.py</path>
        <kind>service</kind>
        <symbol>JobService.update_job_status</symbol>
        <lines>415-483</lines>
        <reason>Helper method for updating job status and progress. Automatically invalidates Redis cache on updates. Signature: update_job_status(job_id, status, progress, stage_metadata, error_message) -> bool</reason>
      </file>
      <file>
        <path>backend/app/core/celery_app.py</path>
        <kind>config</kind>
        <symbol>celery_app</symbol>
        <reason>Celery application instance configured with Redis broker. Use for task registration and configuration.</reason>
      </file>
      <file>
        <path>backend/app/services/storage/supabase_storage.py</path>
        <kind>service</kind>
        <symbol>SupabaseStorageService</symbol>
        <reason>Service for Supabase Storage operations: download_file(), upload_file(), generate_signed_url(), delete_file(). Used for PDF input/output file management.</reason>
      </file>
      <file>
        <path>backend/app/api/v1/upload.py</path>
        <kind>api_endpoint</kind>
        <symbol>upload_pdf</symbol>
        <reason>Upload endpoint that dispatches the conversion workflow chain. Reference for how to launch chain from API: chain(...).apply_async()</reason>
      </file>
      <file>
        <path>backend/tests/unit/tasks/test_conversion_pipeline.py</path>
        <kind>test</kind>
        <symbol>test_convert_to_html, test_extract_content, etc.</symbol>
        <reason>Existing unit tests for pipeline tasks. Use as reference for test patterns and mocking strategies.</reason>
      </file>
    </code>
    <dependencies>
      <python>
        <dep name="celery" version="5.5.3">Distributed task queue for async processing</dep>
        <dep name="redis" version="5.0.1">Message broker and result backend for Celery</dep>
        <dep name="httpx">Async HTTP client used by StirlingPDFClient</dep>
        <dep name="supabase" version="2.24.0">Supabase Python client for database and storage</dep>
        <dep name="langchain" version="0.3.12">AI orchestration framework</dep>
        <dep name="beautifulsoup4">HTML parsing in extract_content task</dep>
        <dep name="pymupdf">PDF metadata extraction in identify_structure task</dep>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    - Architecture: Pipeline / Pipes and Filters pattern using Celery Canvas chain
    - Infrastructure: Celery Workers + Redis Broker + Stirling-PDF Service (Railway)
    - External Service: Stirling-PDF (Docker container on Railway, internal URL via STIRLING_PDF_URL)
    - Database: Persist critical status to Supabase PostgreSQL (conversion_jobs table), use Redis for transient state and caching
    - Story Scope: Orchestration layer only. This story USES existing implementations (StirlingPDFClient is ready, tasks are implemented). Focus on understanding, testing, and potentially enhancing existing code.
    - Tasks must be idempotent: Can be retried safely without side effects
    - Error handling: Permanent errors (InvalidPDFError, CorruptedFileError) should NOT retry. Transient errors (network, timeout) should retry with exponential backoff.
    - Status updates: MUST invalidate Redis cache after every database update
  </constraints>

  <interfaces>
    <interface>
      <name>convert_to_html</name>
      <kind>celery_task</kind>
      <signature>@celery_app.task def convert_to_html(self, job_id: str) -> Dict[str, Any]</signature>
      <path>backend/app/tasks/conversion_pipeline.py</path>
      <description>Stage 1: Convert PDF to HTML using StirlingPDFClient. Downloads PDF from Supabase Storage, calls Stirling API, stores HTML in database, updates job status to CONVERTING (40% progress).</description>
    </interface>
    <interface>
      <name>extract_content</name>
      <kind>celery_task</kind>
      <signature>@celery_app.task def extract_content(self, previous_result: Dict[str, Any]) -> Dict[str, Any]</signature>
      <path>backend/app/tasks/conversion_pipeline.py</path>
      <description>Stage 2: Parse and clean HTML content. Uses BeautifulSoup to sanitize HTML, count elements (images, tables, paragraphs), extract text for AI analysis. Returns cleaned_html and extraction_stats.</description>
    </interface>
    <interface>
      <name>identify_structure</name>
      <kind>celery_task</kind>
      <signature>@celery_app.task def identify_structure(self, previous_result: Dict[str, Any]) -> Dict[str, Any]</signature>
      <path>backend/app/tasks/conversion_pipeline.py</path>
      <description>Stage 3: AI structure analysis using GPT-4o. Extracts document title, author, TOC, chapters, language. Uses StructureAnalyzer with fallback to HeuristicStructureDetector. Updates job status to STRUCTURING (75% progress).</description>
    </interface>
    <interface>
      <name>generate_epub</name>
      <kind>celery_task</kind>
      <signature>@celery_app.task def generate_epub(self, previous_result: Dict[str, Any]) -> Dict[str, Any]</signature>
      <path>backend/app/tasks/conversion_pipeline.py</path>
      <description>Stage 4: Generate EPUB 3.0 file from analyzed content. Uses EpubGenerator to create EPUB with AI-detected structure, uploads to Supabase Storage downloads bucket, generates signed URL, updates job status to COMPLETED (100% progress).</description>
    </interface>
    <interface>
      <name>calculate_quality_score</name>
      <kind>celery_task</kind>
      <signature>@celery_app.task def calculate_quality_score(self, previous_result: Dict[str, Any]) -> Dict[str, Any]</signature>
      <path>backend/app/tasks/conversion_pipeline.py</path>
      <description>Stage 5: Calculate quality metrics from AI analysis. Uses QualityScorer to aggregate confidence scores, validates against PRD targets (95%+ complex, 99%+ text). Stores quality_report in database.</description>
    </interface>
    <interface>
      <name>update_job_status</name>
      <kind>function</kind>
      <signature>def update_job_status(job_id: str, status: str, progress: int = None, stage_metadata: Dict[str, Any] = None, error_message: str = None) -> None</signature>
      <path>backend/app/tasks/conversion_pipeline.py</path>
      <description>Helper function for updating job status in database and invalidating Redis cache. Used by all pipeline tasks to track progress.</description>
    </interface>
    <interface>
      <name>check_cancellation</name>
      <kind>function</kind>
      <signature>def check_cancellation(job_id: str) -> bool</signature>
      <path>backend/app/tasks/conversion_pipeline.py</path>
      <description>Check if job has been cancelled by user. Raises TaskCancelled exception if deleted_at field is set. Called at start of each task.</description>
    </interface>
    <interface>
      <name>StirlingPDFClient.convert_pdf_to_html</name>
      <kind>async_method</kind>
      <signature>async def convert_pdf_to_html(self, pdf_bytes: bytes, filename: str = "document.pdf") -> str</signature>
      <path>backend/app/services/stirling/stirling_client.py</path>
      <description>Convert PDF to HTML via Stirling-PDF API. Sends multipart/form-data POST to /api/v1/convert/pdf/html. Returns HTML string. Timeout: 300 seconds.</description>
    </interface>
  </interfaces>

  <tests>
    <standards>
      - Unit tests: Mock external services (StirlingPDFClient, Supabase, Redis) using pytest fixtures
      - Task testing: Use Celery test client with CELERY_TASK_ALWAYS_EAGER=True for synchronous execution
      - Mock patterns: Mock httpx.AsyncClient for Stirling API responses, mock Supabase client methods
      - Integration tests: Use real Stirling-PDF service (if deployed) OR recorded HTTP responses (VCR.py)
      - Error scenarios: Test retry logic, permanent error handling, cancellation checking
      - Coverage target: 80% minimum for pipeline tasks
    </standards>
    <locations>
      - backend/tests/unit/tasks/test_conversion_pipeline.py (Existing unit tests)
      - backend/tests/integration/test_pipeline_orchestration.py (Integration test for full chain)
      - backend/tests/fixtures/ (Sample PDFs and mock responses)
    </locations>
    <ideas>
      - Test AC1: Verify chain executes tasks in correct order (convert -> extract -> structure -> generate -> qa)
      - Test AC2: Verify job status updates occur at each stage transition with correct progress percentages
      - Test AC3: Test StirlingPDFClient timeout handling (mock 5-minute timeout, verify retry logic)
      - Test AC4: Test automatic retry for transient failures (network error, simulate 3 retries with exponential backoff)
      - Test AC4: Test global error handler marks job as FAILED with correct error message
      - Test pipeline cancellation: Simulate user cancelling job mid-conversion, verify task chain stops
      - Test Redis cache invalidation: Verify cache is cleared after each status update
      - Manual smoke test: Upload real 5-page PDF, verify end-to-end flow completes, check database for correct status transitions
    </ideas>
  </tests>
</story-context>
